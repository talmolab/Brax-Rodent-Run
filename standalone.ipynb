{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e522e226576607c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T23:24:29.200325510Z",
     "start_time": "2024-01-07T23:24:28.080429452Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import functools\n",
    "from IPython.display import HTML\n",
    "import jax\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Sequence, Tuple, Union\n",
    "import wandb\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.envs.base import Env, PipelineEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "from etils import epath\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from ml_collections import config_dict\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "\n",
    "import yaml\n",
    "from typing import List, Dict, Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a299caada632eabb",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b838f89cf541ec7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## TODO:\n",
    "\n",
    "- Check the healthy z-range of the rodent. Now the training\n",
    "    - Check mj_data and how to pull out kinematics of the simulations\n",
    "- Check the `brax.envs` and how I can pass the custom parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79737af2-cfc3-43f2-8bba-23560901e963",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Humanoid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 127\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# external_contact_forces are excluded\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jp\u001b[38;5;241m.\u001b[39mconcatenate([\n\u001b[1;32m    119\u001b[0m         position,\n\u001b[1;32m    120\u001b[0m         data\u001b[38;5;241m.\u001b[39mqvel,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m         data\u001b[38;5;241m.\u001b[39mqfrc_actuator,\n\u001b[1;32m    124\u001b[0m     ])\n\u001b[0;32m--> 127\u001b[0m envs\u001b[38;5;241m.\u001b[39mregister_environment(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumanoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mHumanoid\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Humanoid' is not defined"
     ]
    }
   ],
   "source": [
    "#@title Humanoid Env\n",
    "\n",
    "class Rodent(PipelineEnv):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      forward_reward_weight=1.25,\n",
    "      ctrl_cost_weight=0.1,\n",
    "      healthy_reward=5.0,\n",
    "      terminate_when_unhealthy=True,\n",
    "      healthy_z_range=(0.70, 0.0),\n",
    "      reset_noise_scale=1e-2,\n",
    "      exclude_current_positions_from_observation=True,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    params = load_params(\"params/params.yaml\")\n",
    "    mj_model = mujoco.MjModel.from_xml_path(params[\"XML_PATH\"])\n",
    "    mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "    mj_model.opt.iterations = 6\n",
    "    mj_model.opt.ls_iterations = 6\n",
    "\n",
    "    sys = mjcf.load_model(mj_model)\n",
    "\n",
    "    physics_steps_per_control_step = 5\n",
    "    kwargs['n_frames'] = kwargs.get(\n",
    "        'n_frames', physics_steps_per_control_step\n",
    "    )\n",
    "    kwargs['backend'] = 'mjx'\n",
    "\n",
    "    super().__init__(sys, **kwargs)\n",
    "\n",
    "    self._forward_reward_weight = forward_reward_weight\n",
    "    self._ctrl_cost_weight = ctrl_cost_weight\n",
    "    self._healthy_reward = healthy_reward\n",
    "    self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "    self._healthy_z_range = healthy_z_range\n",
    "    self._reset_noise_scale = reset_noise_scale\n",
    "    self._exclude_current_positions_from_observation = (\n",
    "        exclude_current_positions_from_observation\n",
    "    )\n",
    "\n",
    "  def reset(self, rng: jp.ndarray) -> State:\n",
    "    \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "    rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "    low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "    qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "        rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "    )\n",
    "    qvel = jax.random.uniform(\n",
    "        rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "    )\n",
    "\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    obs = self._get_obs(data, jp.zeros(self.sys.nu))\n",
    "    reward, done, zero = jp.zeros(3)\n",
    "    metrics = {\n",
    "        'forward_reward': zero,\n",
    "        'reward_linvel': zero,\n",
    "        'reward_quadctrl': zero,\n",
    "        'reward_alive': zero,\n",
    "        'x_position': zero,\n",
    "        'y_position': zero,\n",
    "        'distance_from_origin': zero,\n",
    "        'x_velocity': zero,\n",
    "        'y_velocity': zero,\n",
    "    }\n",
    "    return State(data, obs, reward, done, metrics)\n",
    "\n",
    "  def step(self, state: State, action: jp.ndarray) -> State:\n",
    "    \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "    data0 = state.pipeline_state\n",
    "    data = self.pipeline_step(data0, action)\n",
    "\n",
    "    com_before = data0.subtree_com[1]\n",
    "    com_after = data.subtree_com[1]\n",
    "    velocity = (com_after - com_before) / self.dt\n",
    "    forward_reward = self._forward_reward_weight * velocity[0]\n",
    "\n",
    "    min_z, max_z = self._healthy_z_range\n",
    "    is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
    "    if self._terminate_when_unhealthy:\n",
    "      healthy_reward = self._healthy_reward\n",
    "    else:\n",
    "      healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "    ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "    obs = self._get_obs(data, action)\n",
    "    reward = forward_reward + healthy_reward - ctrl_cost\n",
    "    done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "    state.metrics.update(\n",
    "        forward_reward=forward_reward,\n",
    "        reward_linvel=forward_reward,\n",
    "        reward_quadctrl=-ctrl_cost,\n",
    "        reward_alive=healthy_reward,\n",
    "        x_position=com_after[0],\n",
    "        y_position=com_after[1],\n",
    "        distance_from_origin=jp.linalg.norm(com_after),\n",
    "        x_velocity=velocity[0],\n",
    "        y_velocity=velocity[1],\n",
    "    )\n",
    "\n",
    "    return state.replace(\n",
    "        pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "\n",
    "  def _get_obs(\n",
    "      self, data: mjx.Data, action: jp.ndarray\n",
    "  ) -> jp.ndarray:\n",
    "    \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
    "    position = data.qpos\n",
    "    if self._exclude_current_positions_from_observation:\n",
    "      position = position[2:]\n",
    "\n",
    "    # external_contact_forces are excluded\n",
    "    return jp.concatenate([\n",
    "        position,\n",
    "        data.qvel,\n",
    "        data.cinert[1:].ravel(),\n",
    "        data.cvel[1:].ravel(),\n",
    "        data.qfrc_actuator,\n",
    "    ])\n",
    "\n",
    "\n",
    "envs.register_environment('rodent', Rodent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386c700a9aa7ed57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T23:24:29.219972768Z",
     "start_time": "2024-01-07T23:24:29.200175009Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def load_params(param_path: Text) -> Dict:\n",
    "    with open(param_path, \"rb\") as file:\n",
    "        params = yaml.safe_load(file)\n",
    "    return params\n",
    "\n",
    "\n",
    "params = load_params(\"params/params.yaml\")\n",
    "\n",
    "class Rodent(MjxEnv):\n",
    "    \n",
    "    # Might want to change the terminate_when_unhealthy params to enables\n",
    "    # longer episode length, since the average episode length is too short (1 timestep)\n",
    "    # temp change the `terminate_when_unhealthy` to extend the episode length.\n",
    "    def __init__(\n",
    "            self,\n",
    "            forward_reward_weight=5,\n",
    "            ctrl_cost_weight=0.1,\n",
    "            healthy_reward=1.0,\n",
    "            terminate_when_unhealthy=False,\n",
    "            healthy_z_range=(0.0, 2.0),\n",
    "            reset_noise_scale=1e-2,\n",
    "            exclude_current_positions_from_observation=False,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        params = load_params(\"params/params.yaml\")\n",
    "        mj_model = mujoco.MjModel.from_xml_path(params[\"XML_PATH\"])\n",
    "        mj_model.opt.solver = mujoco.mjtSolver.mjSOL_CG\n",
    "        mj_model.opt.iterations = 6\n",
    "        mj_model.opt.ls_iterations = 6\n",
    "\n",
    "        physics_steps_per_control_step = 5\n",
    "        kwargs['n_frames'] = kwargs.get(\n",
    "            'n_frames', physics_steps_per_control_step)\n",
    "\n",
    "        super().__init__(model=mj_model, **kwargs)\n",
    "\n",
    "        self._forward_reward_weight = forward_reward_weight\n",
    "        self._ctrl_cost_weight = ctrl_cost_weight\n",
    "        self._healthy_reward = healthy_reward\n",
    "        self._terminate_when_unhealthy = terminate_when_unhealthy\n",
    "        self._healthy_z_range = healthy_z_range\n",
    "        self._reset_noise_scale = reset_noise_scale\n",
    "        self._exclude_current_positions_from_observation = (\n",
    "            exclude_current_positions_from_observation\n",
    "        )\n",
    "\n",
    "    def reset(self, rng: jp.ndarray) -> State:\n",
    "        \"\"\"Resets the environment to an initial state.\"\"\"\n",
    "        rng, rng1, rng2 = jax.random.split(rng, 3)\n",
    "\n",
    "        low, hi = -self._reset_noise_scale, self._reset_noise_scale\n",
    "        qpos = self.sys.qpos0 + jax.random.uniform(\n",
    "            rng1, (self.sys.nq,), minval=low, maxval=hi\n",
    "        )\n",
    "        qvel = jax.random.uniform(\n",
    "            rng2, (self.sys.nv,), minval=low, maxval=hi\n",
    "        )\n",
    "\n",
    "        data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "        obs = self._get_obs(data.data, jp.zeros(self.sys.nu))\n",
    "        reward, done, zero = jp.zeros(3)\n",
    "        metrics = {\n",
    "            'forward_reward': zero,\n",
    "            'reward_linvel': zero,\n",
    "            'reward_quadctrl': zero,\n",
    "            'reward_alive': zero,\n",
    "            'x_position': zero,\n",
    "            'y_position': zero,\n",
    "            'distance_from_origin': zero,\n",
    "            'x_velocity': zero,\n",
    "            'y_velocity': zero,\n",
    "        }\n",
    "        return State(data, obs, reward, done, metrics)\n",
    "\n",
    "    def step(self, state: State, action: jp.ndarray) -> State:\n",
    "        \"\"\"Runs one timestep of the environment's dynamics.\"\"\"\n",
    "        data0 = state.pipeline_state\n",
    "        data = self.pipeline_step(data0, action)\n",
    "        # based on the timestep simulation, calculate the rewards\n",
    "        com_before = data0.data.subtree_com[1]\n",
    "        com_after = data.data.subtree_com[1]\n",
    "        velocity = (com_after - com_before) / self.dt\n",
    "        forward_reward = self._forward_reward_weight * velocity[0]\n",
    "\n",
    "        min_z, max_z = self._healthy_z_range\n",
    "        is_healthy = jp.where(data.q[2] < min_z, 0.0, 1.0)\n",
    "        is_healthy = jp.where(data.q[2] > max_z, 0.0, is_healthy)\n",
    "        if self._terminate_when_unhealthy:\n",
    "            healthy_reward = self._healthy_reward\n",
    "        else:\n",
    "            healthy_reward = self._healthy_reward * is_healthy\n",
    "\n",
    "        ctrl_cost = self._ctrl_cost_weight * jp.sum(jp.square(action))\n",
    "\n",
    "        obs = self._get_obs(data.data, action)\n",
    "        reward = forward_reward + healthy_reward - ctrl_cost\n",
    "        # terminates when unhealthy\n",
    "        done = 1.0 - is_healthy if self._terminate_when_unhealthy else 0.0\n",
    "        state.metrics.update(\n",
    "            forward_reward=forward_reward,\n",
    "            reward_linvel=forward_reward,\n",
    "            reward_quadctrl=-ctrl_cost,\n",
    "            reward_alive=healthy_reward,\n",
    "            x_position=com_after[0],\n",
    "            y_position=com_after[1],\n",
    "            distance_from_origin=jp.linalg.norm(com_after),\n",
    "            x_velocity=velocity[0],\n",
    "            y_velocity=velocity[1],\n",
    "        )\n",
    "\n",
    "        return state.replace(\n",
    "            pipeline_state=data, obs=obs, reward=reward, done=done\n",
    "        )\n",
    "\n",
    "    def _get_obs(\n",
    "            self, data: mjx.Data, action: jp.ndarray\n",
    "    ) -> jp.ndarray:\n",
    "        \"\"\"Observes humanoid body position, velocities, and angles.\"\"\"\n",
    "        position = data.qpos\n",
    "        if self._exclude_current_positions_from_observation:\n",
    "            position = position[2:]\n",
    "            \n",
    "        # external_contact_forces are excluded\n",
    "        return jp.concatenate([\n",
    "            position,\n",
    "            data.qvel,\n",
    "            data.cinert[1:].ravel(),\n",
    "            data.cvel[1:].ravel(),\n",
    "            data.qfrc_actuator,\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f82e2-8ac7-4cdb-b411-09b5a25b2ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7f036e22165a894",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18840c74ea1dce43",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-07T23:24:29.909178299Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myuy004\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/talmolab/Desktop/Salk-Research/online_training/wandb/run-20240108_110354-i72dwl5y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yuy004/vnl/runs/i72dwl5y' target=\"_blank\">cosmic-dream-18</a></strong> to <a href='https://wandb.ai/yuy004/vnl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yuy004/vnl' target=\"_blank\">https://wandb.ai/yuy004/vnl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yuy004/vnl/runs/i72dwl5y' target=\"_blank\">https://wandb.ai/yuy004/vnl/runs/i72dwl5y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 11:19:03.220956: W external/tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "RESOURCE_EXHAUSTED: Out of memory while trying to allocate 4358373376 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m     metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_steps\n\u001b[1;32m     43\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog(metrics)\n\u001b[0;32m---> 46\u001b[0m make_inference_fn, params, _\u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvironment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/stac-mjx/lib/python3.12/site-packages/brax/training/agents/ppo/train.py:423\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(environment, num_timesteps, episode_length, action_repeat, num_envs, max_devices_per_host, num_eval_envs, learning_rate, entropy_cost, discounting, seed, unroll_length, batch_size, num_minibatches, num_updates_per_batch, num_evals, num_resets_per_eval, normalize_observations, reward_scaling, clipping_epsilon, gae_lambda, deterministic_eval, network_factory, progress_fn, normalize_advantage, eval_env, policy_params_fn, randomization_fn)\u001b[0m\n\u001b[1;32m    420\u001b[0m epoch_key, local_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(local_key)\n\u001b[1;32m    421\u001b[0m epoch_keys \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(epoch_key, local_devices_to_use)\n\u001b[1;32m    422\u001b[0m (training_state, env_state, training_metrics) \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 423\u001b[0m     \u001b[43mtraining_epoch_with_timing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m )\n\u001b[1;32m    425\u001b[0m current_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(_unpmap(training_state\u001b[38;5;241m.\u001b[39menv_steps))\n\u001b[1;32m    427\u001b[0m key_envs \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x, s: jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(x[\u001b[38;5;241m0\u001b[39m], s),\n\u001b[1;32m    429\u001b[0m     in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))(key_envs, key_envs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/stac-mjx/lib/python3.12/site-packages/brax/training/agents/ppo/train.py:350\u001b[0m, in \u001b[0;36mtrain.<locals>.training_epoch_with_timing\u001b[0;34m(training_state, env_state, key)\u001b[0m\n\u001b[1;32m    348\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    349\u001b[0m training_state, env_state \u001b[38;5;241m=\u001b[39m _strip_weak_type((training_state, env_state))\n\u001b[0;32m--> 350\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m training_state, env_state, metrics \u001b[38;5;241m=\u001b[39m _strip_weak_type(result)\n\u001b[1;32m    353\u001b[0m metrics \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree_util\u001b[38;5;241m.\u001b[39mtree_map(jnp\u001b[38;5;241m.\u001b[39mmean, metrics)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/stac-mjx/lib/python3.12/site-packages/jax/_src/compiler.py:261\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    257\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 4358373376 bytes."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "envs.register_environment('rodent', Rodent)\n",
    "\n",
    "# instantiate the environment\n",
    "env_name = 'rodent'\n",
    "env = envs.get_environment(env_name)\n",
    "\n",
    "# define the jit reset/step functions\n",
    "jit_reset = jax.jit(env.reset)\n",
    "jit_step = jax.jit(env.step)\n",
    "\n",
    "config = {\n",
    "    \"env_name\": env_name,\n",
    "    \"algo_name\": \"ppo\",\n",
    "    \"task_name\": \"run\",\n",
    "    \"num_envs\": 1024,\n",
    "    \"num_timesteps\": 10_000_000,\n",
    "    \"eval_every\": 10_000,\n",
    "    \"episode_length\": 1000,\n",
    "    \"num_evals\": 1000,\n",
    "    \"batch_size\": 256,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"terminate_when_unhealthy\": False\n",
    "}\n",
    "\n",
    "\n",
    "train_fn = functools.partial(\n",
    "    ppo.train, num_timesteps=config[\"num_timesteps\"], num_evals=int(config[\"num_timesteps\"]/config[\"eval_every\"]),\n",
    "    reward_scaling=0.1, episode_length=config[\"episode_length\"], normalize_observations=True, action_repeat=1,\n",
    "    unroll_length=10, num_minibatches=8, num_updates_per_batch=4,\n",
    "    discounting=0.98, learning_rate=config[\"learning_rate\"], entropy_cost=1e-3, num_envs=config[\"num_envs\"],\n",
    "    batch_size=config[\"batch_size\"], seed=0)\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"vnl\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "wandb.run.name = f\"{config[\"env_name\"]}_{config['task_name']}_{config[\"algo_name\"]}_brax\"\n",
    "\n",
    "\n",
    "def wandb_progress(num_steps, metrics):\n",
    "    metrics[\"num_steps\"] = num_steps\n",
    "    wandb.log(metrics)\n",
    "    \n",
    "\n",
    "make_inference_fn, params, _= train_fn(environment=env, progress_fn=wandb_progress)\n",
    "\n",
    "\n",
    "#@title Save Model\n",
    "model_path = '/cps/brax_ppo_rodent_run'\n",
    "model.save_params(model_path, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769ed88e5ddec36",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "inference_fn = make_inference_fn(params)\n",
    "jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "eval_env = envs.get_environment(env_name)\n",
    "\n",
    "jit_reset = jax.jit(eval_env.reset)\n",
    "jit_step = jax.jit(eval_env.step)\n",
    "\n",
    "# initialize the state\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = jit_reset(rng)\n",
    "rollout = [state.pipeline_state]\n",
    "\n",
    "# grab a trajectory\n",
    "n_steps = 500\n",
    "render_every = 1\n",
    "\n",
    "for i in range(n_steps):\n",
    "  act_rng, rng = jax.random.split(rng)\n",
    "  ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "  state = jit_step(state, ctrl)\n",
    "  rollout.append(state.pipeline_state)\n",
    "\n",
    "  if state.done:\n",
    "    break\n",
    "\n",
    "media.show_video(env.render(rollout[::render_every], camera='side'), fps=1.0 / env.dt / render_every)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
